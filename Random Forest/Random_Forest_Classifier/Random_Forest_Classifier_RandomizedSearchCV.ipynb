{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem: We are given information about a subset of the Titanic population and asked to build a predictive model that tells us whether or not a given passenger survived the shipwreck. We are given 10 basic explanatory variables, including passenger gender, age, and price of fare, among others. This is a classic binary classification problem, and we will be implementing a random forest classifer.\n",
    "\n",
    "#### Data: Kaggle - Titanic: Machine Learning from Disaster https://www.kaggle.com/c/titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a huge difference between classifiers and regressors. Classifiers predict one class from a predetermined list or probabilities of belonging to a class. Regressors predict some value, which could be almost anything.\n",
    "Different metrics are used for classification and regression.\n",
    "#### Difference between a Classifier and a Regressor.\n",
    "A <b>Classifier</b> is used to predict a set of specified labels - The simplest( and most hackneyed) example being that of Email Spam Detection where we will always want to classify whether an email is either spam (1) or not spam(0) .  So each email will get either a 0 or 1 or maybe even a fraction(if you go ahead and decide to predict the probability of an email being spam. \n",
    "<br />\n",
    "Examples : Classifying movie reviews based on text, Detecting the presence of seizures from recorded EEG signals, Classifying whether a passenger would survive in the Titanic disaster.\n",
    "<br /><br />\n",
    "On the other hand a <b>Regressor</b> is used to predict real valued outputs which vary and dont require outputs predicted to be in a fixed set. For example when wanting to predict the future income of restaurants, we really dont know all the possible outputs.\n",
    "<br />\n",
    "Examples : Predicting future Elo ratings of Chess players, Predicting the runs scored by a team in a cricket match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Inspections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Cabin</td>\n",
       "      <td>687</td>\n",
       "      <td>77.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Age</td>\n",
       "      <td>177</td>\n",
       "      <td>19.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Embarked</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Fare</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Ticket</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Total     %\n",
       "Cabin       687  77.1\n",
       "Age         177  19.9\n",
       "Embarked      2   0.2\n",
       "Fare          0   0.0\n",
       "Ticket        0   0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = train.isnull().sum().sort_values(ascending=False)\n",
    "percent_1 = train.isnull().sum()/train.isnull().count()*100\n",
    "percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n",
    "missing_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embarked feature has only 2 missing values, which can easily be filled. It will be much more tricky, to deal with the 'Age' feature, which has 177 missing values. The 'Cabin' feature needs further investigation, but it looks like that we might want to drop it from the dataset, since 77 % of it are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that 38% out of the training-set survived the Titanic. We can also see that the passenger ages range from 0.4 to 80. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Survived column is already binary format. Name, Sex, Ticket, Cabin,and Embarked need further adaptation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.616162\n",
       "1    0.383838\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Survived'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that 62% of the people in the training set died. This is slightly less than the estimated 67% that died in the actual shipwreck (1500/2224)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass\n",
       "1    0.629630\n",
       "2    0.472826\n",
       "3    0.242363\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Survived'].groupby(train['Pclass']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class played a critical role in survival, the survival rate decreased drastically for the lowest class. This variable is both useful and clean, and is categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name & Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Name column as provided cannot be used in the model. However, we might be able to extract some meaningful information from it. First, we can obtain useful information about the passenger's title. Looking at the distribution of the titles, it might be useful to group the smaller sized values into an 'other' group, but this example we don't do it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr.          517\n",
       "Miss.        182\n",
       "Mrs.         125\n",
       "Master.       40\n",
       "Dr.            7\n",
       "Rev.           6\n",
       "Col.           2\n",
       "Major.         2\n",
       "Mlle.          2\n",
       "Lady.          1\n",
       "Capt.          1\n",
       "Jonkheer.      1\n",
       "Ms.            1\n",
       "Don.           1\n",
       "Mme.           1\n",
       "Sir.           1\n",
       "the            1\n",
       "Name: Name_Title, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Name_Title'] = train['Name'].apply(lambda x: x.split(',')[1]).apply(lambda x: x.split()[0])\n",
    "train['Name_Title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name_Title\n",
       "Capt.        0.000000\n",
       "Col.         0.500000\n",
       "Don.         0.000000\n",
       "Dr.          0.428571\n",
       "Jonkheer.    0.000000\n",
       "Lady.        1.000000\n",
       "Major.       0.500000\n",
       "Master.      0.575000\n",
       "Miss.        0.697802\n",
       "Mlle.        1.000000\n",
       "Mme.         1.000000\n",
       "Mr.          0.156673\n",
       "Mrs.         0.792000\n",
       "Ms.          1.000000\n",
       "Rev.         0.000000\n",
       "Sir.         1.000000\n",
       "the          1.000000\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Survived'].groupby(train['Name_Title']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the survival rate appears to be either significantly above or below the average survival rate, this variable should be able to help our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male      0.647587\n",
       "female    0.352413\n",
       "Name: Sex, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Sex'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sex\n",
       "female    0.742038\n",
       "male      0.188908\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Survived'].groupby(train['Sex']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Women and children first,\" goes the famous saying. Thus, we should expect females to have a higher survival rate than males, and indeed that is the case. We expect this variable to be very useful in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age\n",
       "False    0.406162\n",
       "True     0.293785\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Survived'].groupby(train['Age'].isnull()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age\n",
       "(0.419, 19.0]    0.481707\n",
       "(19.0, 25.0]     0.328467\n",
       "(25.0, 31.8]     0.393701\n",
       "(31.8, 41.0]     0.437500\n",
       "(41.0, 80.0]     0.373239\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Survived'].groupby(pd.qcut(train['Age'],5)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.419, 19.0]    164\n",
       "(31.8, 41.0]     144\n",
       "(41.0, 80.0]     142\n",
       "(19.0, 25.0]     137\n",
       "(25.0, 31.8]     127\n",
       "Name: Age, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.qcut(train['Age'],5).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon first glance, the relationship between age and survival appears to be a murky one at best. However, this doesn't mean that the variable will be a bad predictor; at deeper levels of a given decision tree, a more discriminant relationship might open up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SibSp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SibSp\n",
       "0    0.345395\n",
       "1    0.535885\n",
       "2    0.464286\n",
       "3    0.250000\n",
       "4    0.166667\n",
       "5    0.000000\n",
       "8    0.000000\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Survived'].groupby(train['SibSp']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    608\n",
       "1    209\n",
       "2     28\n",
       "4     18\n",
       "3     16\n",
       "8      7\n",
       "5      5\n",
       "Name: SibSp, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['SibSp'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon first glance, the importance of this variable is not too convincing. The distribution and survival rate between the different categories does not provide much hope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parch\n",
       "0    0.343658\n",
       "1    0.550847\n",
       "2    0.500000\n",
       "3    0.600000\n",
       "4    0.000000\n",
       "5    0.200000\n",
       "6    0.000000\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Survived'].groupby(train['Parch']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    678\n",
       "1    118\n",
       "2     80\n",
       "5      5\n",
       "3      5\n",
       "4      4\n",
       "6      1\n",
       "Name: Parch, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Parch'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "passengers with zero parents or children had a lower likelihood of survival than otherwise, but that survival rate was only slightly less than the overall population survival rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have two seemingly weak predictors, one thing we can do is combine them to get a stronger predictor. In the case of SibSp and Parch, we can combine the two variables to get a 'family size' metric, which might (and in fact does) prove to be a better predictor than the two original variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ticket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ticket column seems to contain unique alphanumeric values, and is thus not very useful on its own. However, we might be able to extract come predictive power from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           A/5 21171\n",
       "1            PC 17599\n",
       "2    STON/O2. 3101282\n",
       "3              113803\n",
       "4              373450\n",
       "5              330877\n",
       "6               17463\n",
       "7              349909\n",
       "8              347742\n",
       "9              237736\n",
       "Name: Ticket, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Ticket'].head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One piece of potentially useful informatin is the number of characters in the Ticket column. This could be a reflection of the 'type' of ticket a given passenger had, which could somehow indicate their chances of survival. One theory (which may in fact be verifiable) is that some characteristic of the ticket could indicate the location of the passenger's room, which might be a crucial factor in their escape route, and consequently their survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Ticket_Len'] = train['Ticket'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6     419\n",
       "5     131\n",
       "4     101\n",
       "8      76\n",
       "10     41\n",
       "7      27\n",
       "9      26\n",
       "17     14\n",
       "16     11\n",
       "13     10\n",
       "12     10\n",
       "15      9\n",
       "11      8\n",
       "18      6\n",
       "3       2\n",
       "Name: Ticket_Len, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Ticket_Len'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another piece of information is the first letter of each ticket, which, again, might be indicative of a certain attribute of the ticketholders or their rooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Ticket_Letter'] = train['Ticket'].apply(lambda x: str(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    301\n",
       "2    183\n",
       "1    146\n",
       "S     65\n",
       "P     65\n",
       "C     47\n",
       "A     29\n",
       "W     13\n",
       "4     10\n",
       "7      9\n",
       "F      7\n",
       "6      6\n",
       "L      4\n",
       "5      3\n",
       "8      2\n",
       "9      1\n",
       "Name: Ticket_Letter, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Ticket_Letter'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ticket_Letter\n",
       "1    0.630137\n",
       "2    0.464481\n",
       "3    0.239203\n",
       "4    0.200000\n",
       "5    0.000000\n",
       "6    0.166667\n",
       "7    0.111111\n",
       "8    0.000000\n",
       "9    1.000000\n",
       "A    0.068966\n",
       "C    0.340426\n",
       "F    0.571429\n",
       "L    0.250000\n",
       "P    0.646154\n",
       "S    0.323077\n",
       "W    0.153846\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(['Ticket_Letter'])['Survived'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.001, 8.662]    308\n",
       "(26.0, 512.329]    295\n",
       "(8.662, 26.0]      288\n",
       "Name: Fare, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.qcut(train['Fare'], 3).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fare\n",
       "(-0.001, 8.662]    0.198052\n",
       "(8.662, 26.0]      0.402778\n",
       "(26.0, 512.329]    0.559322\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Survived'].groupby(pd.qcut(train['Fare'], 3)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear relationship between Fare and Survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pclass</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>(-0.001, 7.854]</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(7.854, 10.5]</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(10.5, 21.679]</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(21.679, 39.688]</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(39.688, 512.329]</td>\n",
       "      <td>146</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pclass               1   2    3\n",
       "Fare                           \n",
       "(-0.001, 7.854]      6   6  167\n",
       "(7.854, 10.5]        0  24  160\n",
       "(10.5, 21.679]       0  80   92\n",
       "(21.679, 39.688]    64  64   52\n",
       "(39.688, 512.329]  146  10   20"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(pd.qcut(train['Fare'], 5), columns=train['Pclass'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear relationship between Fare and Class too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cabin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This column has the most nulls (almost 700), but we can still extract information from it, like the first letter of each cabin, or the cabin number. The usefulness of this column might be similar to that of the Ticket variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cabin letter\n",
    "\n",
    "We can see that most of the cabin letters are associated with a high survival rate, so this might very well be a useful variable. Because there aren't that many unique values, we won't do any grouping here, even if some of the values have a small count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Cabin_Letter'] = train['Cabin'].apply(lambda x: str(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n    687\n",
       "C     59\n",
       "B     47\n",
       "D     33\n",
       "E     32\n",
       "A     15\n",
       "F     13\n",
       "G      4\n",
       "T      1\n",
       "Name: Cabin_Letter, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Cabin_Letter'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cabin_Letter\n",
       "A    0.466667\n",
       "B    0.744681\n",
       "C    0.593220\n",
       "D    0.757576\n",
       "E    0.750000\n",
       "F    0.615385\n",
       "G    0.500000\n",
       "T    0.000000\n",
       "n    0.299854\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Survived'].groupby(train['Cabin_Letter']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cabin Number\n",
    "\n",
    "Upon first glance, this appears to be useless. Not only do we have ~700 nulls which will be difficult to impute, but the correlation with Survived is almost zero. However, the cabin numbers as a whole do seem to have a high surival rate compared to the population average, so we might want to keep this just in case for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Cabin_num'] = train['Cabin'].apply(lambda x: str(x).split(' ')[-1][1:])\n",
    "train['Cabin_num'].replace('an', np.NaN, inplace = True)\n",
    "train['Cabin_num'] = train['Cabin_num'].apply(lambda x: int(x) if not pd.isnull(x) and x != '' else np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65.667, 148.0]     67\n",
       "(1.999, 28.667]     67\n",
       "(28.667, 65.667]    66\n",
       "Name: Cabin_num, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.qcut(train['Cabin_num'],3).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cabin_num\n",
       "(1.999, 28.667]     0.716418\n",
       "(28.667, 65.667]    0.651515\n",
       "(65.667, 148.0]     0.641791\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Survived'].groupby(pd.qcut(train['Cabin_num'], 3)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.06384595922789371"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Survived'].corr(train['Cabin_num'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embarked\n",
    "Looks like the Cherbourg people had a 20% higher survival rate than the other embarking locations. This is very likely due to the high presence of upper-class passengers from that location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S    644\n",
       "C    168\n",
       "Q     77\n",
       "Name: Embarked, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Embarked'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S    0.724409\n",
       "C    0.188976\n",
       "Q    0.086614\n",
       "Name: Embarked, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Embarked'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embarked\n",
       "C    0.553571\n",
       "Q    0.389610\n",
       "S    0.336957\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Survived'].groupby(train['Embarked']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x188b5620048>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(train['Embarked'], hue=train['Pclass'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done our cursory exploration of the variables, we now have a pretty good idea of how we want to transform our variables in preparation for our final dataset. We will perform our feature engineering through a series of helper functions that each serve a specific purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first function creates two separate columns: a numeric column indicating the length of a passenger's Name field, and a categorical column that extracts the passenger's title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names(train, test):\n",
    "    for i in [train, test]:\n",
    "        i['Name_Len'] = i['Name'].apply(lambda x: len(x))\n",
    "        i['Name_Title'] = i['Name'].apply(lambda x: x.split(',')[1]).apply(lambda x: x.split()[0])\n",
    "        del i['Name']\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we impute the null values of the Age column by filling in the mean value of the passenger's corresponding title and class. This more granular approach to imputation should be more accurate than merely taking the mean age of the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_impute(train, test):\n",
    "    for i in [train, test]:\n",
    "        i['Age_Null_Flag'] = i['Age'].apply(lambda x: 1 if pd.isnull(x) else 0)\n",
    "        data = train.groupby(['Name_Title', 'Pclass'])['Age']\n",
    "        i['Age'] = data.transform(lambda x: x.fillna(x.mean()))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the SibSp and Parch columns into a new variable that indicates family size, and group the family size variable into three categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fam_size(train, test):\n",
    "    for i in [train, test]:\n",
    "        i['Fam_Size'] = np.where((i['SibSp']+i['Parch']) == 0 , 'Solo',\n",
    "                           np.where((i['SibSp']+i['Parch']) <= 3,'Nuclear', 'Big'))\n",
    "        del i['SibSp']\n",
    "        del i['Parch']\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ticket column is used to create two new columns: Ticket_Letter, which indicates the first letter of each ticket (with the smaller-n values being grouped based on survival rate); and Ticket_Len, which indicates the length of the Ticket field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticket_grouped(train, test):\n",
    "    for i in [train, test]:\n",
    "        i['Ticket_Letter'] = i['Ticket'].apply(lambda x: str(x)[0])\n",
    "        i['Ticket_Letter'] = i['Ticket_Letter'].apply(lambda x: str(x))\n",
    "        i['Ticket_Letter'] = np.where((i['Ticket_Letter']).isin(['1', '2', '3', 'S', 'P', 'C', 'A']), i['Ticket_Letter'],\n",
    "                                   np.where((i['Ticket_Letter']).isin(['W', '4', '7', '6', 'L', '5', '8']),\n",
    "                                            'Low_ticket', 'Other_ticket'))\n",
    "        i['Ticket_Len'] = i['Ticket'].apply(lambda x: len(x))\n",
    "        del i['Ticket']\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two functions extract the first letter of the Cabin column and its number, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cabin(train, test):\n",
    "    for i in [train, test]:\n",
    "        i['Cabin_Letter'] = i['Cabin'].apply(lambda x: str(x)[0])\n",
    "        del i['Cabin']\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cabin_num(train, test):\n",
    "    for i in [train, test]:\n",
    "        i['Cabin_num1'] = i['Cabin'].apply(lambda x: str(x).split(' ')[-1][1:])\n",
    "        i['Cabin_num1'].replace('an', np.NaN, inplace = True)\n",
    "        i['Cabin_num1'] = i['Cabin_num1'].apply(lambda x: int(x) if not pd.isnull(x) and x != '' else np.NaN)\n",
    "        i['Cabin_num'] = pd.qcut(train['Cabin_num1'],3)\n",
    "    train = pd.concat((train, pd.get_dummies(train['Cabin_num'], prefix = 'Cabin_num')), axis = 1)\n",
    "    test = pd.concat((test, pd.get_dummies(test['Cabin_num'], prefix = 'Cabin_num')), axis = 1)\n",
    "    del train['Cabin_num']\n",
    "    del test['Cabin_num']\n",
    "    del train['Cabin_num1']\n",
    "    del test['Cabin_num1']\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fill the null values in the Embarked column with the most commonly occuring value, which is 'S.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embarked_impute(train, test):\n",
    "    for i in [train, test]:\n",
    "        i['Embarked'] = i['Embarked'].fillna('S')\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also fill in the one missing value of Fare in our test set with the mean value of Fare from the training set (transformations of test set data must always be fit using training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Fare'].fillna(train['Fare'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, because we are using scikit-learn, we must convert our categorical columns into dummy variables. The following function does this, and then it drops the original categorical columns. It also makes sure that each category is present in both the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummies(train, test, columns = ['Pclass', 'Sex', 'Embarked', 'Ticket_Lett', 'Cabin_Letter', 'Name_Title', 'Fam_Size']):\n",
    "    for column in columns:\n",
    "        train[column] = train[column].apply(lambda x: str(x))\n",
    "        test[column] = test[column].apply(lambda x: str(x))\n",
    "        good_cols = [column+'_'+i for i in train[column].unique() if i in test[column].unique()]\n",
    "        train = pd.concat((train, pd.get_dummies(train[column], prefix = column)[good_cols]), axis = 1)\n",
    "        test = pd.concat((test, pd.get_dummies(test[column], prefix = column)[good_cols]), axis = 1)\n",
    "        del train[column]\n",
    "        del test[column]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our last helper function drops any columns that haven't already been dropped. In our case, we only need to drop the PassengerId column, which we have decided is not useful for our problem (by the way, I've confirmed this with a separate test). Note that dropping the PassengerId column here means that we'll have to load it later when creating our submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop(train, test, bye = ['PassengerId']):\n",
    "    for i in [train, test]:\n",
    "        for z in bye:\n",
    "            del i[z]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having built our helper functions, we can now execute them in order to build our dataset that will be used in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "train, test = names(train, test)\n",
    "train, test = age_impute(train, test)\n",
    "train, test = cabin_num(train, test)\n",
    "train, test = cabin(train, test)\n",
    "train, test = embarked_impute(train, test)\n",
    "train, test = fam_size(train, test)\n",
    "test['Fare'].fillna(train['Fare'].mean(), inplace = True)\n",
    "train, test = ticket_grouped(train, test)\n",
    "train, test = dummies(train, test, columns = ['Pclass', 'Sex', 'Embarked', 'Ticket_Letter',\n",
    "                                                                     'Cabin_Letter', 'Name_Title', 'Fam_Size'])\n",
    "train, test = drop(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "print(len(train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our final dataset has 45 columns, composed of our target column and 44 predictor variables. Although highly dimensional datasets can result in high variance but let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Name_Len</th>\n",
       "      <th>Age_Null_Flag</th>\n",
       "      <th>Cabin_num_(1.999, 28.667]</th>\n",
       "      <th>Cabin_num_(28.667, 65.667]</th>\n",
       "      <th>Cabin_num_(65.667, 148.0]</th>\n",
       "      <th>Ticket_Len</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>...</th>\n",
       "      <th>Name_Title_Mrs.</th>\n",
       "      <th>Name_Title_Miss.</th>\n",
       "      <th>Name_Title_Master.</th>\n",
       "      <th>Name_Title_Rev.</th>\n",
       "      <th>Name_Title_Dr.</th>\n",
       "      <th>Name_Title_Ms.</th>\n",
       "      <th>Name_Title_Col.</th>\n",
       "      <th>Fam_Size_Nuclear</th>\n",
       "      <th>Fam_Size_Solo</th>\n",
       "      <th>Fam_Size_Big</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived   Age     Fare  Name_Len  Age_Null_Flag  \\\n",
       "0         0  22.0   7.2500        23              0   \n",
       "1         1  38.0  71.2833        51              0   \n",
       "2         1  26.0   7.9250        22              0   \n",
       "3         1  35.0  53.1000        44              0   \n",
       "4         0  35.0   8.0500        24              0   \n",
       "\n",
       "   Cabin_num_(1.999, 28.667]  Cabin_num_(28.667, 65.667]  \\\n",
       "0                          0                           0   \n",
       "1                          0                           0   \n",
       "2                          0                           0   \n",
       "3                          0                           0   \n",
       "4                          0                           0   \n",
       "\n",
       "   Cabin_num_(65.667, 148.0]  Ticket_Len  Pclass_3  ...  Name_Title_Mrs.  \\\n",
       "0                          0           9         1  ...                0   \n",
       "1                          1           8         0  ...                1   \n",
       "2                          0          16         1  ...                0   \n",
       "3                          1           6         0  ...                1   \n",
       "4                          0           6         1  ...                0   \n",
       "\n",
       "   Name_Title_Miss.  Name_Title_Master.  Name_Title_Rev.  Name_Title_Dr.  \\\n",
       "0                 0                   0                0               0   \n",
       "1                 0                   0                0               0   \n",
       "2                 1                   0                0               0   \n",
       "3                 0                   0                0               0   \n",
       "4                 0                   0                0               0   \n",
       "\n",
       "   Name_Title_Ms.  Name_Title_Col.  Fam_Size_Nuclear  Fam_Size_Solo  \\\n",
       "0               0                0                 1              0   \n",
       "1               0                0                 1              0   \n",
       "2               0                0                 0              1   \n",
       "3               0                0                 1              0   \n",
       "4               0                0                 0              1   \n",
       "\n",
       "   Fam_Size_Big  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid vs. Random Search:\n",
    "<br/><br/>\n",
    "In contrast to model parameters which are learned during training, model hyperparameters are set by the data scientist ahead of training and control implementation aspects of the model. Hyperparameters can be thought of as model settings. These settings need to be tuned because the ideal settings for one data set will not be the same across all data sets. When tuning the hyperparameters of an estimator, Grid Search and Random Search are both popular methods.\n",
    "<br/><br/>\n",
    "Grid Search can be thought of as an exhaustive search for selecting a model. In Grid Search, the data scientist sets up a grid of hyperparameter values and for each combination, trains a model and scores on the testing data. In this approach, every combination of hyperparameter values is tried which can be very inefficient. For example, searching 20 different parameter values for each of 4 parameters will require 160,000 trials of cross-validation. This equates to 1,600,000 model fits and 1,600,000 predictions if 10-fold cross validation is used. While Scikit Learn offers the GridSearchCV function to simplify the process, it would be an extremely costly execution both in computing power and time.\n",
    "<br/><br/>\n",
    "By contrast, Random Search sets up a grid of hyperparameter values and selects random combinations to train the model and score. This allows you to explicitly control the number of parameter combinations that are attempted. The number of search iterations is set based on time or resources. Scikit Learn offers the RandomizedSearchCV function for this process.\n",
    "While it’s possible that RandomizedSearchCV will not find as accurate of a result as GridSearchCV, it surprisingly picks the best result more often than not and in a fraction of the time it takes GridSearchCV would have taken. Given the same resources, Randomized Search can even outperform Grid Search. This can be visualized in the graphic below when continuous parameters are used.\n",
    "<br/><br/>\n",
    "Model tuning is the process of finding the best machine learning model hyperparameters for a particular data set. Random and Grid Search are two uniformed methods for hyperparameter tuning and Scikit Learn offers these functions through GridSearchCV and RandomizedSearchCV.\n",
    "<br/><br/>\n",
    "With small data sets and lots of resources, Grid Search will produce accurate results. However, with large data sets, the high dimensions will greatly slow down computation time and be very costly. In this instance, it is advised to use Randomized Search since the number of iterations is explicitly defined by the data scientist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'criterion': 'gini', 'max_depth': 10, 'max_features': 1, 'min_samples_leaf': 1, 'n_estimators': 300}\n",
      "0.8372615039281706\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1, oob_score=True)\n",
    "\n",
    "rf_p_dist={'max_depth':[3,5,10,None],\n",
    "              'n_estimators':[10,100,200,300,400,500],\n",
    "              'max_features':randint(1,3),\n",
    "               'criterion':['gini','entropy'],\n",
    "               'bootstrap':[True,False],\n",
    "               'min_samples_leaf':randint(1,4),\n",
    "              }\n",
    "\n",
    "rdmsearch = RandomizedSearchCV(rf, rf_p_dist, n_jobs=-1, n_iter=40, cv=9)\n",
    "\n",
    "rscv = rdmsearch.fit(train.iloc[:, 1:], train.iloc[:, 0])\n",
    "\n",
    "print(rscv.best_params_)\n",
    "print(rscv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Estimation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8316\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_jobs=-1, \n",
    "                            n_estimators=300,\n",
    "                            bootstrap= True,\n",
    "                            criterion='gini',\n",
    "                            max_depth=10,\n",
    "                            max_features=1,\n",
    "                            oob_score=True,\n",
    "                            min_samples_leaf= 1)\n",
    "\n",
    "rf.fit(train.iloc[:, 1:], train.iloc[:, 0])\n",
    "print(\"%.4f\" % rf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf.predict(test)\n",
    "predictions = pd.DataFrame(predictions, columns=['Survived'])\n",
    "test = pd.read_csv('test.csv')\n",
    "predictions = pd.concat((test.iloc[:, 0], predictions), axis = 1)\n",
    "predictions.to_csv('y_test1.csv', sep=\",\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
